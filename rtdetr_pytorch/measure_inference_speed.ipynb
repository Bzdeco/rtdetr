{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:53:11.530041Z",
     "start_time": "2024-04-26T15:53:06.347601Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:53:42.438986Z",
     "start_time": "2024-04-26T15:53:36.149288Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load PResNet18 state_dict\n"
     ]
    }
   ],
   "source": [
    "from tools.train import rt_detr_config\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = rt_detr_config().model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:54:21.755537Z",
     "start_time": "2024-04-26T15:54:21.254930Z"
    }
   },
   "outputs": [],
   "source": [
    "from powerlines.sahi import multiscale_image_patches\n",
    "\n",
    "# Construct random test input\n",
    "# scales = [256, 512, 1024]\n",
    "scales = [512, 1024]\n",
    "\n",
    "original_input = torch.randn((1, 3, 3000, 4096), dtype=torch.float)\n",
    "patches = multiscale_image_patches(\n",
    "    original_input, patch_sizes=scales, step_size_fraction=0.8, predict_on_full_image=True\n",
    ")\n",
    "num_patch_inputs = len(patches.patches)\n",
    "num_batches_per_frame = num_patch_inputs / 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6578947368421053"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "min_batch_size = 2\n",
    "max_batch_size = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutating batch size 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  25%|██▍       | 246/1000 [00:27<01:23,  9.02it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size_step = 4\n",
    "# batch_sizes = np.linspace(min_batch_size, max_batch_size, num=int((max_batch_size - min_batch_size) / batch_size_step + 1)).astype(int)\n",
    "batch_sizes = [38]\n",
    "means, stds = [], []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Evalutating batch size {batch_size}\")\n",
    "    \n",
    "    input = torch.randn((batch_size, 3, 640, 640), dtype=torch.float).to(device)\n",
    "    num_batches_per_frame = num_patch_inputs / batch_size\n",
    "\n",
    "    # Timers\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = int(1000)\n",
    "    warm_up_rounds = 50\n",
    "    timings = torch.zeros((repetitions, 1))\n",
    "\n",
    "    # Run on GPU\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "        # GPU warm-up\n",
    "        for _ in range(warm_up_rounds):\n",
    "            _ = model(input)\n",
    "\n",
    "        # Measure inference time\n",
    "        with torch.no_grad():\n",
    "            for rep in tqdm(range(repetitions), desc=\"Inference\"):\n",
    "                starter.record()\n",
    "                _ = model(input)\n",
    "                ender.record()\n",
    "\n",
    "                # Synchronize GPU\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time = starter.elapsed_time(ender)\n",
    "                timings[rep] = elapsed_time\n",
    "\n",
    "    # Compute inference time (per entire frame)\n",
    "    mean_syn = timings.mean() * num_batches_per_frame\n",
    "    std_syn = timings.std() * num_batches_per_frame\n",
    "    means.append(mean_syn)\n",
    "    stds.append(std_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(290.1152)], [tensor(0.6649)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finding optimal batch size\n",
    "idx = torch.as_tensor(means).argmin()\n",
    "optimal_batch_size = batch_sizes[idx.item()]\n",
    "means[idx.item()], stds[idx.item()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
